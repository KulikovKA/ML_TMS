{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01cac51c",
   "metadata": {},
   "source": [
    "# ***Теория графов.Основные понятия и определения.***\n",
    "## Граф.Основные понятия.\n",
    "\n",
    "**Граф** - это топологичекая *модель*, которая состоит из *множества вершин* и *множества соединяющих их рёбер*. При этом значение имеет только сам факт, какая вершина с какой соединена. \n",
    "<br></br>Иначе говоря : **Граф (G)**: пара множеств **G = (V, E)**, где **V** — множество вершин (узлов), а **E** — множество рёбер (связей)\n",
    "<br>\n",
    "<div align=\"left\">\n",
    "<img src=\"./theory_of_graphs/ffb024567fd7f560044a3927f8a7aab0.jpg\" width=\"600\" height=\"400\" align=\"center\">\n",
    "<div/>\n",
    "<div align = 'left'>\n",
    "\n",
    "**Вершина (Узел)** - отдельный объект в графе.\n",
    "\n",
    "**Ребро** - неупорядоченная пара вершин, обозначающая связь.\n",
    "\n",
    "**Инцидентность** - связь между вершиной и ребром. Вершина инцидентна ребру, если является его концом.\n",
    "\n",
    "**Смежность:**\n",
    "\n",
    "- *Смежные вершины*: Соединены одним ребром.\n",
    "\n",
    "- *Смежные рёбра*: Имеют общую вершину.\n",
    "<div/>\n",
    "<br><br/>\n",
    "<div align=\"left\">\n",
    "<img src=\"./theory_of_graphs/смежность.jpg\" width=\"600\" height=\"400\" align=\"center\">\n",
    "<div/>\n",
    "<div align = 'left'>\n",
    "<br><br/>\n",
    "\n",
    "## Виды графов.Степени вершин.\n",
    "\n",
    "**Петля** - ребро, инцидентное одной вершине. Ребро, которое замыкается на одной вершине.\n",
    "\n",
    "**Псевдограф** - граф с петлями.\n",
    "<div/>\n",
    "<br><br/>\n",
    "<div align=\"left\">\n",
    "<img src=\"./theory_of_graphs/петля.png\" width=\"600\" height=\"400\" align=\"center\">\n",
    "<div/>\n",
    "<div align = 'left'>\n",
    "<br><br/>\n",
    "\n",
    "**Кратные рёбра** - рёбра, имеющие одинаковые концевые вершины, по другому их называют ещё параллельными.\n",
    "\n",
    "**Мультиграф** - граф с кратными рёбрами.\n",
    "\n",
    "**Псевдомультиграф** - граф с петлями и кратными рёбрами.\n",
    "<br><br/>\n",
    "<div align=\"left\">\n",
    "<img src=\"./theory_of_graphs/мультиграф.jpg\" width=\"600\" height=\"400\" align=\"center\">\n",
    "<div/>\n",
    "<div align = 'left'>\n",
    "<br><br/>\n",
    "\n",
    "**Степень вершины** - это количество рёбер, инцидентных указанной вершине. По-другому - количество рёбер, исходящих из вершины. Петля увеливает степень вершины на 2.\n",
    "\n",
    "**Изолированная вершина** - вершина с нулевой степенью.\n",
    "\n",
    "**Висячая вершина** - вершина со степенью 1.\n",
    "<br><br/>\n",
    "<div align=\"left\">\n",
    "<img src=\"./theory_of_graphs/степень вершины.jpg\" width=\"600\" height=\"400\" align=\"center\">\n",
    "<div/>\n",
    "<div align = 'left'>\n",
    "<br><br/>\n",
    "\n",
    "**Подграф** : Если в исходном графе выделить несколько вершин и несколько рёбер (между выбранными вершинами), то мы получим подграф исходного графа.\n",
    "<br><br/>\n",
    "<div align=\"left\">\n",
    "<img src=\"./theory_of_graphs/подграф.png\" width=\"600\" height=\"400\" align=\"center\">\n",
    "<div/>\n",
    "<div align = 'left'>\n",
    "<br><br/>\n",
    "\n",
    "**Полный граф** - это граф, в котором каждые две вершины соединены одним ребром.Обзначаются как **K_{n}**, где *n* - кол-во вершин в графе. \n",
    "<br><br/>*Граф на рисунке имеет 7 вершин => K_7*\n",
    "<br><br/>\n",
    "<div align=\"left\">\n",
    "<img src=\"./theory_of_graphs/Полный граф.png\" width=\"600\" height=\"400\" align=\"center\">\n",
    "<div/>\n",
    "<div align = 'left'>\n",
    "<br><br/>\n",
    "\n",
    "***Задача о рукопожатия:***\n",
    "- собралось N человек (вершин) и каждый с каждым обменялся рукопожатием (ребро), сколько всего было рукопожатий(ребер в полном графе)?\n",
    "\n",
    "***Решение***\n",
    "- Вычисляется как сумма чисел от 1 до N - каждый новый участник должен пожать руку всем присутствующим, вычисляется по формуле: **N * (N - 1) / 2**\n",
    "\n",
    "*Кол-во вершин в полном графе* - **N * (N - 1) / 2**, где N - число вершин.\n",
    "\n",
    "<br><br/>\n",
    "**Регулярный граф** - граф, в котором степени всех вершин одинаковые.\n",
    "<br><br/>\n",
    "<div align=\"left\">\n",
    "<img src=\"./theory_of_graphs/регулярный граф.png\" width=\"600\" height=\"400\" align=\"center\">\n",
    "<div/>\n",
    "<div align = 'left'>\n",
    "<br><br/>\n",
    "\n",
    "**Двудольный граф** - если все вершины графа можно разделить на **два множества** таким образом, что каждое ребро соединяет вершины из разных множеств, то такой граф называется двудольным. Например, клиент-серверное приложение содержит множество запросов (рёбер) между клиентом и сервером, но нет запросов внутри клиента или внутри сервера.\n",
    "<br><br/>\n",
    "*Обозначается как* **K_{n}_{m}**, где *n* - кол-во вершин из 1-ого множества, а *m* - кол-во вершин из 2-ого множества.\n",
    "\n",
    "<br><br/>\n",
    "<div align=\"left\">\n",
    "<img src=\"./theory_of_graphs/Двудольный граф.png\" width=\"600\" height=\"400\" align=\"center\">\n",
    "<div/>\n",
    "<div align = 'left'>\n",
    "<br><br/>\n",
    "\n",
    "**Планарный граф** : Если граф можно разместить на плоскости таким образом, чтобы рёбра не пересекались, то он называется “планарным графом” или “плоским графом”.\n",
    "\n",
    "<br><br/>\n",
    "<div align=\"left\">\n",
    "<img src=\"./theory_of_graphs/Планарный граф.png\" width=\"600\" height=\"400\" align=\"center\">\n",
    "<div/>\n",
    "<div align = 'left'>\n",
    "<br><br/>\n",
    "\n",
    "**Непланырный граф** - граф,ребра которого невозможно расположить на плоскости без пересечений.\n",
    "Минимальные непланарные графы - это полный граф К5 из 5 вершин и полный двудольный граф К3,3 из 3+3 вершин. Если какой-либо граф в качестве подграфа содержит К5 или К3,3, то он является непланарным.\n",
    "<br><br/>\n",
    "<div align=\"left\">\n",
    "<img src=\"./theory_of_graphs/Непланарный граф.png\" width=\"600\" height=\"400\" align=\"center\">\n",
    "<div/>\n",
    "<div align = 'left'>\n",
    "<br><br/>\n",
    "\n",
    "## Цилкы и пути.\n",
    "\n",
    "**Путь или Маршрут** - это последовательность смежных рёбер. Обычно путь задаётся перечислением вершин, по которым он пролегает.\n",
    "\n",
    "**Длина пути** - количество рёбер в пути.\n",
    "\n",
    "**Цепь** - маршрут без повторяющихся рёбер.\n",
    "\n",
    "**Простая цепь** - цепь без повторяющихся вершин.\n",
    "\n",
    "<br><br/>\n",
    "<div align=\"left\">\n",
    "<img src=\"./theory_of_graphs/Путь.jpg\" width=\"600\" height=\"400\" align=\"center\">\n",
    "<div/>\n",
    "<div align = 'left'>\n",
    "<br><br/>\n",
    "\n",
    "**Цикл или Контур** - цепь, в котором последняя вершина совпадает с первой. \n",
    "\n",
    "**Длина цикла** - количество рёбер в цикле.\n",
    "\n",
    "**Самый короткий цикл** - это петля.\n",
    "\n",
    "<br><br/>\n",
    "<div align=\"left\">\n",
    "<img src=\"./theory_of_graphs/Цикл.png\" width=\"600\" height=\"400\" align=\"center\">\n",
    "<div/>\n",
    "<div align = 'left'>\n",
    "<br><br/>\n",
    "\n",
    "**Цикл Эйлера** - цикл, проходящий по каждому ребру ровно один раз. Эйлер доказал, что такой цикл существует тогда, и только тогда, когда все вершины в связанном графе имеют чётную степень.\n",
    "*Иначе говоря* - Пусть **G=(V,E)** — связный неориентированный граф. \n",
    "**(∃ цикл Эйлера в G) ⇔ (∀ v ∈ V, deg (v) mod2 = 0).**\n",
    "\n",
    "<br><br/>\n",
    "<div align=\"left\">\n",
    "<img src=\"./theory_of_graphs/цикл эйлера.jpg\" width=\"600\" height=\"400\" align=\"center\">\n",
    "<div/>\n",
    "<div align = 'left'>\n",
    "<br><br/>\n",
    "\n",
    "**Цикл Гамильтона** - цикл, проходящий через все вершины графа по одному разу. Другими словами - это простой цикл, в который входят все вершины графа.\n",
    "\n",
    "<br><br/>\n",
    "<div align=\"left\">\n",
    "<img src=\"./theory_of_graphs/цикл гамильтоны.jpg\" width=\"600\" height=\"400\" align=\"center\">\n",
    "<div/>\n",
    "<div align = 'left'>\n",
    "<br><br/>\n",
    "\n",
    "## Взвешенный граф.Дерево и лес.\n",
    "\n",
    "**Взвешенный граф** - граф, в котором у каждого ребра и/или каждой вершины есть **“вес”** - некоторое число, которое может обозначать длину пути, его стоимость и т. п.\n",
    "\n",
    "<br><br/>\n",
    "<div align=\"left\">\n",
    "<img src=\"./theory_of_graphs/взвешенный граф.jpg\" width=\"600\" height=\"400\" align=\"center\">\n",
    "<div/>\n",
    "<div align = 'left'>\n",
    "<br><br/>\n",
    "\n",
    "**Связный граф** - граф, в котором существует путь между любыми двумия вершинами.\n",
    "\n",
    "**Дерево** - связный граф без циклов.\n",
    "\n",
    "*Между любыми двумя вершинами дерева существует единственный путь.*\n",
    "\n",
    "**Лес** - граф, в котором несколько деревьев.\n",
    "\n",
    "\n",
    "<br><br/>\n",
    "<div style=\"display: flex; justify-content: space-between; align-items: center;\">\n",
    "    <div style=\"text-align: center;\">\n",
    "        <img src=\"./theory_of_graphs/дерево.jpg\" width=\"600\" height=\"400\"><br>\n",
    "        <i>Дерево</i>\n",
    "    </div>\n",
    "    <div style=\"text-align: center;\">\n",
    "        <img src=\"./theory_of_graphs/лес.jpg\" width=\"600\" height=\"400\"><br>\n",
    "        <i>Лес</i>\n",
    "    </div>\n",
    "</div>\n",
    "<br><br/>\n",
    "\n",
    "## Оргграф.Компоненты связности.Мост.\n",
    "\n",
    "**Ориентированный граф или Орграф** - граф, в котором рёбра имеют направления.\n",
    "\n",
    "**Дуга** - направленные рёбра в ориентированном графе.\n",
    "\n",
    "<br><br/>\n",
    "<div align=\"left\">\n",
    "<img src=\"./theory_of_graphs/оргграф.jpg\" width=\"600\" height=\"400\" align=\"center\">\n",
    "<div/>\n",
    "<div align = 'left'>\n",
    "<br><br/>\n",
    "\n",
    "**Полустепень захода вершины** - количество дуг, заходящих в эту вершину.\n",
    "\n",
    "**Исток** - вершина с нулевой полустепенью захода.\n",
    "\n",
    "**Полустепень исхода вершины** - количество дуг, исходящих из этой вершины\n",
    "\n",
    "**Сток** - вершина с нулевой полустепенью исхода.\n",
    "\n",
    "<br><br/>\n",
    "<div align=\"left\">\n",
    "<img src=\"./theory_of_graphs/сток.png\" width=\"600\" height=\"400\" align=\"center\">\n",
    "<div/>\n",
    "<div align = 'left'>\n",
    "<br><br/>\n",
    "\n",
    "**Компонента связности** - множество таких вершин графа, что между любыми двумя вершинами существует маршрут.\n",
    "\n",
    "<br><br/>\n",
    "<div align=\"left\">\n",
    "<img src=\"./theory_of_graphs/компонента связности.jpg\" width=\"600\" height=\"400\" align=\"center\">\n",
    "<div/>\n",
    "<div align = 'left'>\n",
    "<br><br/>\n",
    "\n",
    "**Компонента сильной связности** - максимальное множество вершин орграфа, между любыми двумя вершинами которого существует **путь по дугам**.\n",
    "\n",
    "**Компонента слабой связности** - максимальное множество вершин орграфа, между любыми двумя вершинами которого существует **путь по дугам без учёта направления** (по дугам можно двигаться в любом направлении **(по сути путь по ребрам)**).\n",
    "\n",
    "<br><br/>\n",
    "<div align=\"left\">\n",
    "<img src=\"./theory_of_graphs/компонента сильной связности.jpg\" width=\"600\" height=\"400\" align=\"center\">\n",
    "<div/>\n",
    "<div align = 'left'>\n",
    "<br><br/>\n",
    "\n",
    "**Мост** - ребро, при удалении которого, количество связанных компонент графа увеличивается.\n",
    "\n",
    "<br><br/>\n",
    "<div align=\"left\">\n",
    "<img src=\"./theory_of_graphs/мост.jpg\" width=\"600\" height=\"400\" align=\"center\">\n",
    "<div/>\n",
    "<div align = 'left'>\n",
    "<br><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6360fc19",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f003bf3",
   "metadata": {},
   "source": [
    "\n",
    "# **Алгоритмы анализа графов: описание и применение**\n",
    "\n",
    "\n",
    "\n",
    "## 1. Обнаружение сообществ (Community Detection)\n",
    "\n",
    "### Как работает\n",
    "Алгоритмы обнаруживают плотно связанные группы узлов, где связи внутри группы значительно плотнее, чем связи между разными группами. Используются метрики модулярности для оценки качества разделения.\n",
    "\n",
    "**Основные алгоритмы:**\n",
    "- **Лувена (Louvain)** - жадная оптимизация модулярности\n",
    "- **Лейдена (Leiden)** - улучшенная версия с гарантией связности\n",
    "- **Label Propagation** - распространение меток между соседями\n",
    "\n",
    "### Применение\n",
    "- **Социальные сети**: выявление групп по интересам, профессиональных сообществ\n",
    "- **Рекомендательные системы**: группировка пользователей с похожими предпочтениями\n",
    "- **Биоинформатика**: идентификация функциональных модулей в белковых сетях\n",
    "- **Безопасность**: обнаружение преступных группировок по паттернам связей\n",
    "\n",
    "## 2. Алгоритмы центральности (Centrality Algorithms)\n",
    "\n",
    "### Как работает\n",
    "Измеряют влияние и важность узлов в сети через различные метрики:\n",
    "\n",
    "**Типы центральности:**\n",
    "- **Степень (Degree)**: количество непосредственных связей\n",
    "- **Посредничество (Betweenness)**: частота нахождения на кратчайших путях\n",
    "- **Близость (Closeness)**: среднее расстояние до всех других узлов\n",
    "- **Собственный вектор (Eigenvector)**: важность с учетом важности соседей\n",
    "\n",
    "### Применение\n",
    "- **Маркетинг**: идентификация лидеров мнений для продвижения\n",
    "- **Управление рисками**: поиск ключевых сотрудников в организационной структуре\n",
    "- **Логистика**: оптимизация критических узлов в транспортных сетях\n",
    "- **ML-признаки**: создание фич для моделей предсказания оттока клиентов\n",
    "\n",
    "## 3. Алгоритмы предсказания связей (Link Prediction)\n",
    "\n",
    "### Как работает\n",
    "Предсказывают вероятность образования новых связей на основе:\n",
    "- Общих соседей (Common Neighbors)\n",
    "- Метрик сходства (Jaccard, Adamic-Adar)\n",
    "- Матричной факторизации\n",
    "- Графовых нейросетей\n",
    "\n",
    "### Применение\n",
    "- **Социальные сети**: рекомендации друзей и контента\n",
    "- **Электронная коммерция**: \"покупатели также смотрят\"\n",
    "- **Академические исследования**: прогнозирование научных коллабораций\n",
    "- **Здравоохранение**: предсказание взаимодействий белков\n",
    "\n",
    "## 4. Алгоритмы поиска путей (Path Detection)\n",
    "\n",
    "### Как работает\n",
    "Находят оптимальные маршруты между узлами:\n",
    "\n",
    "**Основные алгоритмы:**\n",
    "- **Дейкстры (Dijkstra)** - кратчайшие пути с весами\n",
    "- **A*** - эвристический поиск с приоритетом\n",
    "- **Bellman-Ford** - пути в графах с отрицательными весами\n",
    "\n",
    "### Применение\n",
    "- **Социальный анализ**: \"шесть рукопожатий\" - поиск цепочек знакомств\n",
    "- **Логистика**: оптимизация маршрутов доставки\n",
    "- **Телеком**: прокладка оптимальных сетевых соединений\n",
    "- **Рекомендации**: навигация по связанному контенту\n",
    "\n",
    "## 5. Алгоритмы сходства (Similarity Algorithms)\n",
    "\n",
    "### Как работает\n",
    "Вычисляют меры схожести между узлами на основе:\n",
    "\n",
    "**Типы сходства:**\n",
    "- **Структурное**: общие соседи, коэффициент Жаккара\n",
    "- **Путевое**: индекс Катца, сходство по случайным блужданиям\n",
    "- **Ролевое**: Node2Vec, GraphSAGE - векторные представления\n",
    "\n",
    "### Применение\n",
    "- **Lookalike-аудитории**: поиск пользователей, похожих на целевую группу\n",
    "- **Безопасность**: обнаружение ботов по схожести структурных паттернов\n",
    "- **Рекомендации**: коллаборативная фильтрация\n",
    "- **Фрод-мониторинг**: выявление скоординированных мошеннических схем\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7c4703",
   "metadata": {},
   "source": [
    "# **Графовые модели в  ML/DL**\n",
    "## Эмбендинги.\n",
    "Если начать с простого, то **эмбеддинг** — это массив чисел, который получается преобразованием какого‑то объекта (например текста или картинки).\n",
    "<br><br/>\n",
    "<div align=\"left\">\n",
    "<img src=\"./theory_of_graphs/эмбендинг.png\" width=\"800\" height=\"400\" align=\"center\">\n",
    "<div/>\n",
    "<div align = 'left'>\n",
    "<br><br/>\n",
    "\n",
    "**Векторы (или эмбеддинги)** можно понимать как **точки в некотором пространстве**. В нашем случае, это будут точки в двумерном пространстве. Эти вектора будут иметь 2 координаты [x, y], то есть иметь размерность 2.\n",
    "\n",
    "Представим, что у нас имеется 2 объекта (например текста). После определенного преобразования получаем 2 **двумерных вектора**: [1, 2] и [5, 5] (вектора v1 и v2 соответственно). Эти вектора мы можем разместить в нашей системе координат, поместив начала этих векторов в точку [0, 0].\n",
    "\n",
    "<br><br/>\n",
    "<div align=\"left\">\n",
    "<img src=\"./theory_of_graphs/векторы эмбендингов.png\" width=\"600\" height=\"400\" align=\"center\">\n",
    "<div/>\n",
    "<div align = 'left'>\n",
    "<br><br/>\n",
    "\n",
    "\n",
    "## Классичесские алгоритмы\n",
    "\n",
    "### Deepwalk A\n",
    "\n",
    "**Алгоритм DeepWalk [1]** позволяет схватывать взаимодействия, содержащиеся в графе и программировать их в простых нейронных сетях как векторные представления, которые далее могут потребляться вышеупомянутыми алгоритмами машинного обучения.\n",
    "\n",
    "**DeepWalk** – это тип **графовой нейронной сети [1]**— такой нейронной сети, которая оперирует непосредственно над целевой графовой структурой. Алгоритм использует **прием рандомизированного обхода** пути, чтобы добыть информацию о локализованных структурах внутри сетей. Это делается путем использования таких случайных сетей в качестве последовательностей, в дальнейшем используемых как тренировочное множество для обучения языковой модели Skip-Gram. \n",
    "\n",
    "#### Word2Vec\n",
    "\n",
    "Эта простая реализация алгоритма DeepWalk серьезно полагается на языковую модель Word2Vec [2]. Модель Word2Vec, представленная Google в 2013 году, обеспечивает векторное представление слов в n-мерном пространстве, где похожие слова локализованы поблизости друг от друга. Таким образом, между словами, которые часто используются в сочетании друг с другом или в похожих ситуациях, будут сравнительно невелики косинусные расстояния.\n",
    "- Косинусное расстояние измеряет угол между векторами и оценивает, насколько они направлены параллельно или перпендикулярно, не учитывая длины векторов.\n",
    "\n",
    "<br><br/>\n",
    "<div align=\"left\">\n",
    "<img src=\"./theory_of_graphs/косинус.jpg\" width=\"900\" height=\"350\" align=\"center\">\n",
    "<div/>\n",
    "<div align = 'left'>\n",
    "<br><br/>\n",
    "\n",
    "DeepWalk использует прокладывание **случайных путей по графу**, чтобы выявить в сети скрытые паттерны. Затем модель обучается этим паттернам, и они программируются в нейронных сетях, а на выходе мы получаем наши финальные векторные представления. Генерируются эти случайные пути проще простого: **начиная с целевого узла (корня), случайным образом выбираем узел, соседний данному и достраиваем до него путь.** Далее случайным образом выбираем узел, соседний тому, что нашли на предыдущем шаге, достраиваем до него путь и так далее, пока не будет сделано заданное количество шагов. \n",
    "\n",
    "**Этапы работы DeepWalk:**\n",
    "1. Для каждого узла выполняется N “случайных шагов”, начиная с данного узла.\n",
    "2. Каждый проход трактуется как последовательность строк узел-id.\n",
    "3. Имея список таких последовательностей, обучаем модель word2vec при помощи алгоритма Skip-Gram, учебным множеством для которых служат эти строковые последовательности.\n",
    "\n",
    "\n",
    "<br><br/>\n",
    "<div align=\"left\">\n",
    "<img src=\"./theory_of_graphs/DeepWalk.png\" width=\"400\" height=\"400\" align=\"center\">\n",
    "<div/>\n",
    "<div align = 'left'>\n",
    "<br><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c67ff1e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70c0c0f9",
   "metadata": {},
   "source": [
    "# Конспект: Алгоритм node2vec\n",
    "\n",
    "## Что такое node2vec?\n",
    "**node2vec** — это алгоритм обучения векторных представлений (эмбеддингов) узлов графа, основанный на случайных блужданиях (random walks). Он позволяет эффективно кодировать структурную информацию графа в низкоразмерные векторы, которые можно использовать в задачах машинного обучения (классификация узлов, рекомендации, кластеризация и др.).\n",
    "\n",
    "Разработан как улучшение по сравнению с **DeepWalk**, с более гибким управлением стратегией обхода графа.\n",
    "\n",
    "\n",
    "## Основная идея\n",
    "Алгоритм обучает эмбеддинги так, чтобы узлы, часто встречающиеся в одних и тех же **случайных блужданиях**, имели **похожие векторные представления**.\n",
    "\n",
    "Для этого используется подход, аналогичный **Word2Vec** (Skip-gram):  \n",
    "- Узлы — как слова,  \n",
    "- Случайные блуждания — как предложения.\n",
    "\n",
    "\n",
    "## Ключевое отличие от DeepWalk\n",
    "В **DeepWalk** используются **равновероятные (unbiased)** случайные блуждания: из текущего узла следующий выбирается равновероятно среди соседей.\n",
    "\n",
    "В **node2vec** вводятся два гиперпараметра — **`p`** и **`q`**, которые управляют **смещением** (bias) при выборе следующего шага:\n",
    "\n",
    "| Параметр | Роль |\n",
    "|---------|------|\n",
    "| **`p`** | Вероятность **возврата** к предыдущему узлу. Чем выше `p`, тем меньше склонность к возврату. |\n",
    "| **`q`** | Вероятность **исследования новых областей**. При малом `q` алгоритм предпочитает \"глубокие\" прогулки (DFS-подобные), при большом — \"широкие\" (BFS-подобные). |\n",
    "\n",
    "Это позволяет **балансировать между**:\n",
    "- **Гомофилией** (узлы в одном сообществе похожи → BFS-стиль),\n",
    "- **Структурной эквивалентностью** (узлы с похожей ролью в графе похожи → DFS-стиль).\n",
    "\n",
    "\n",
    "## Архитектура обучения\n",
    "1. **Генерация случайных блужданий**:\n",
    "   - Для каждого узла запускаются блуждания длиной `T`.\n",
    "   - Переходы между узлами регулируются параметрами `p` и `q`.\n",
    "\n",
    "2. **Обучение эмбеддингов**:\n",
    "   - Используется модель **Skip-gram** с **отрицательной выборкой** (negative sampling).\n",
    "   - Цель: максимизировать вероятность появления соседей узла в блуждании:\n",
    "     $$\n",
    "      \\max_f \\sum_{v_i \\in V} \\log P(N_S(v_i) \\mid f(v_i))\n",
    "      $$\n",
    "      где $( f(v_i) $) — эмбеддинг узла $( v_i $), $( N_S(v_i) $) — его окружение в блуждании.\n",
    "\n",
    "3. **Декодер**:\n",
    "   - Восстанавливает вероятность совместного появления узлов через скалярное произведение их эмбеддингов:\n",
    "     $$\n",
    "      \\text{DEC}(z_i, z_j) = \\frac{e^{z_i^\\top z_j}}{\\sum_{v_k \\in V} e^{z_i^\\top z_k}} \\approx p_G(v_j \\mid v_i)\n",
    "      $$\n",
    "   - На практике используется **отрицательная выборка** для ускорения вычислений.\n",
    "\n",
    "\n",
    "## Преимущества\n",
    "- Гибкость: можно настраивать под разные типы структур (сообщества vs роли).\n",
    "- Эффективность: масштабируется на большие графы.\n",
    "- Не требует атрибутов узлов — работает только по топологии.\n",
    "\n",
    "\n",
    "## Ограничения\n",
    "- **Трансдуктивный**: не может генерировать эмбеддинги для новых узлов без переобучения.\n",
    "- Не использует атрибуты узлов (в базовой версии).\n",
    "- Требует подбора гиперпараметров `p` и `q`.\n",
    "\n",
    "\n",
    "\n",
    "## Вывод\n",
    "**node2vec** — мощный и гибкий метод представления узлов графа, сочетающий идеи глубокого обучения и теории графов. Его главная сила — в возможности адаптировать стратегию обхода под конкретную задачу через параметры `p` и `q`, что делает его универсальным инструментом для анализа сетевых данных.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e058534",
   "metadata": {},
   "source": [
    "# **Графовые нейронные сети(GNN)**\n",
    "\n",
    "\n",
    "## Типы задач на графах\n",
    "\n",
    "### 1. Graph-level задачи (уровень всего графа)\n",
    " Классифицировать можно весь граф (graph-level) .В качестве примера graph-level задач можно привести классификацию и регрессию на молекулярных графах. Имея датасет с размеченными молекулами, можно предсказывать их принадлежность к лекарственной категории и различные химико-биологические свойства.\n",
    "\n",
    "### 2. Node-level задачи (уровень вершин)\n",
    " На node-level, как правило, классифицируют вершины одного огромного графа, например, социального. Имея частичную разметку, хочется восстановить метки неразмеченных вершин. Например, предсказать интересы нового пользователя по интересам его друзей.\n",
    "\n",
    "### 3. Edge-level задачи (уровень ребер)\n",
    " Довольно интересна задача предсказания пропущенных связей в графе. В больших графах часто некоторые связи отсутствуют. Например, в социальном графе пользователь может добавить не всех знакомых в друзья.\n",
    "\n",
    "## Принцип работы графовых нейронных сетей\n",
    "\n",
    "### Основная идея\n",
    "В основе GNN заложен механизм распространения информации. Граф обрабатывается набором модулей, которые связаны между собой в соответствии со связями графа. Также каждый из модулей связан с узлами графа. В процессе обучения, модули обновляют свои состояния и обмениваются информацией.\n",
    "\n",
    "\n",
    "<br><br/>\n",
    "<div align=\"left\">\n",
    "<img src=\"./theory_of_graphs/gnn.webp\" width=\"900\" height=\"400\" align=\"center\">\n",
    "<div/>\n",
    "<div align = 'left'>\n",
    "<br><br/>\n",
    "\n",
    "### Математическая формулировка\n",
    " Пусть f — это параметрическая функция, называемая локальной переходной функцией, которая является общей для всех узлов и обновляет состояние узла в соответствии с входной окрестностью. Также пусть g — локальная выходная функция, которая описывает, как выход был производен. Тогда $h_v$ и $o_v$ определяются следующим образом:\n",
    "\n",
    " $h_v = f(x_v, x_{co[v]}, h_{ne[v]}, x_{ne[v]})$\n",
    " $o_v = g(h_v, X_v)$\n",
    "\n",
    " где $x_v, x_{co[v]}, h_{ne[v]}, x_{ne[v]}$ являются признаками $v$, признаками его ребер, состояний, и признаками узлов в окрестностях $v$ соответственно.\"\n",
    "\n",
    "## Классификация графовых сверток\n",
    "\n",
    "### Две основные парадигмы\n",
    "\n",
    "#### 1. Пространственная парадигма (Spatial)\n",
    "Пространственные свертки основываются на message-passing парадигме. Концепт этого подхода заключается в следующем - каждая вершина графа имеет внутреннее состояние. Каждую итерацию это внутреннее состояние пересчитывается, основываясь на внутренних состояниях соседей по графу.\n",
    "\n",
    "**Алгоритм Message Passing:**\n",
    "Для каждой вершины v собираются все тройки $(x_v, x_w, e_{wv})$ состоящие из скрытых представлений текущей вершин $x_v$ и ее соседа $x_w$, а также из типа ребра $e_{wv}$, соединяющего текущую вершину и её соседа. Ко всем этим тройкам применяется обучаемое преобразование M (от слова message), которая считает сообщение — информацию, которая идет от соседа к вершине. Посчитанные сообщения агрегируются в одно, обозначаемое $m_v$. Далее, агрегированное сообщение и текущее внутреннее состояние вершины подаются на вход обучаемой операции U (от слова update), которая обновляет внутреннее состояние вершины.\n",
    "\n",
    "<br><br/>\n",
    "<div align=\"left\">\n",
    "<img src=\"./theory_of_graphs/mp.webp\" width=\"800\" height=\"400\" align=\"center\">\n",
    "<div/>\n",
    "<div align = 'left'>\n",
    "<br><br/>\n",
    "\n",
    "#### 2. Спектральная парадигма (Spectral)\n",
    "Спектральная парадигма опирается на анализ процесса диффузии сигнала внутри графа и анализирует матрицы, описывающих граф — матрицу смежности и матрицу, которая называется **Лапласианом графа**.\n",
    "\n",
    "**Лапласиан графа** — это матрица $L = D - A$, где $D$ — диагональная матрица, хранящая в i-й диагональной ячейке количество исходящих из i-й вершины рёбер, а $A$ — матрица смежности графа.\n",
    "\n",
    "## Популярные архитектуры GNN\n",
    "\n",
    "### Graph Convolutional Network (GCN)\n",
    "Свертка GCN, основанная на спектральной парадигме, использует только скрытые состояния вершин h и матрицу смежности A — она учитывает лишь наличие или отсутствие ребра в графе, но не признаки ребер.\n",
    "\n",
    "<br><br/>\n",
    "<div align=\"left\">\n",
    "<img src=\"./theory_of_graphs/GCN.png\" width=\"900\" height=\"500\" align=\"center\">\n",
    "<div/>\n",
    "<div align = 'left'>\n",
    "<br><br/>\n",
    "\n",
    "\n",
    "**Формула GCN:**\n",
    "$H^{(l+1)} = \\sigma\\left(\\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}H^{(l)}W^{(l)}\\right)$\n",
    "\n",
    "где $\\tilde{A} = A + I$ - матрица смежности с добавленными self-loop'ами, $\\tilde{D}$ - диагональная матрица степеней $\\tilde{A}$\n",
    "\n",
    "---\n",
    "\n",
    "### GraphSAGE\n",
    "Свертка GraphSAGE работает по следующему принципу. Для каждой вершины вычисляется набор скрытых представлений соседних вершин $h_w^t$, из которых идут связи в текущую. Далее, собранная информация агрегируется с помощью некоторой коммутативной операции AGGR в вектор фиксированного размера.\n",
    "\n",
    "<br><br/>\n",
    "<div align=\"left\">\n",
    "<img src=\"./theory_of_graphs/GraphSAGE.png\" width=\"900\" height=\"500\" align=\"center\">\n",
    "<div/>\n",
    "<div align = 'left'>\n",
    "<br><br/>\n",
    "\n",
    "### Формула агрегации:\n",
    "$h_{\\mathcal{N}(v)}^{(l)} = \\text{AGGREGATE}^{(l)}\\left(\\{h_u^{(l-1)}, \\forall u \\in \\mathcal{N}(v)\\}\\right)$\n",
    "\n",
    "### Формула обновления:\n",
    "$h_v^{(l)} = \\sigma\\left(\\mathbf{W}^{(l)} \\cdot \\text{CONCAT}(h_v^{(l-1)}, h_{\\mathcal{N}(v)}^{(l)})\\right)$\n",
    "\n",
    "### Обозначения:\n",
    "\n",
    "#### В формуле агрегации:\n",
    "- **$h_{\\mathcal{N}(v)}^{(l)}$** - **агрегированное представление соседей** узла $v$ на слое $l$\n",
    "- **$\\text{AGGREGATE}^{(l)}$** - **функция агрегации** (обучаемая операция)\n",
    "- **$\\{h_u^{(l-1)}, \\forall u \\in \\mathcal{N}(v)\\}$** - **множество эмбеддингов всех соседей** узла $v$\n",
    "- **$\\mathcal{N}(v)$** - **окрестность узла** $v$ (множество соседей)\n",
    "\n",
    "#### В формуле обновления:\n",
    "- **$h_v^{(l)}$** - **новый эмбеддинг узла** $v$ на слое $l$\n",
    "- **$\\mathbf{W}^{(l)}$** - **обучаемая матрица весов** для слоя $l$\n",
    "- **$\\text{CONCAT}(\\cdot)$** - **конкатенация** векторов\n",
    "- **$h_v^{(l-1)}$** - **предыдущий эмбеддинг узла** $v$\n",
    "- **$\\sigma$** - функция активации (например, ReLU)\n",
    "\n",
    "### Варианты агрегаторов (AGGREGATE):\n",
    "\n",
    "- **Mean aggregator**: усреднение эмбеддингов соседей\n",
    "- **Pooling aggregator**: поэлементный max/mean pooling\n",
    "- **LSTM aggregator**: использование LSTM для последовательной обработки\n",
    "- **GCN aggregator**: как в Graph Convolutional Networks\n",
    "\n",
    "### Пошаговый процесс:\n",
    "\n",
    "1. **Сэмплирование соседей**: выбираем подмножество соседей для каждого узла\n",
    "2. **Агрегация**: объединяем эмбеддинги соседей выбранной функцией\n",
    "3. **Конкатенация**: объединяем собственный эмбеддинг с агрегированным\n",
    "4. **Линейное преобразование**: умножаем на обучаемую матрицу весов\n",
    "5. **Активация**: применяем нелинейную функцию активации\n",
    "\n",
    "\n",
    "\n",
    "**Основное преимущество**: возможность работать с большими графами через сэмплирование и индуктивное обучение.\n",
    "\n",
    "---\n",
    "\n",
    "### Graph Attention Network (GAT)\n",
    "Свертка GAT (Graph ATtention) является развитием идеи GraphSAGE. В качестве механизма агрегации эта архитектура предлагает использовать механизм внимания.\n",
    "\n",
    "<br><br/>\n",
    "<div align=\"left\">\n",
    "<img src=\"./theory_of_graphs/GAT.webp\" width=\"900\" height=\"400\" align=\"center\">\n",
    "<div/>\n",
    "<div align = 'left'>\n",
    "<br><br/>\n",
    "\n",
    "\n",
    "### Формула внимания:\n",
    "$\\alpha_{ij} = \\frac{\\exp\\left(\\text{LeakyReLU}\\left(\\mathbf{a}^T[\\mathbf{W}h_i \\parallel \\mathbf{W}h_j]\\right)\\right)}{\\sum_{k \\in \\mathcal{N}(i)} \\exp\\left(\\text{LeakyReLU}\\left(\\mathbf{a}^T[\\mathbf{W}h_i \\parallel \\mathbf{W}h_k]\\right)\\right)}$\n",
    "\n",
    "### Формула обновления:\n",
    "$h_i^{(l+1)} = \\sigma\\left(\\sum_{j \\in \\mathcal{N}(i)} \\alpha_{ij}\\mathbf{W}h_j^{(l)}\\right)$\n",
    "\n",
    "### Обозначения:\n",
    "\n",
    "#### В формуле внимания:\n",
    "- **$\\alpha_{ij}$** - **коэффициент внимания** между узлами $i$ и $j$\n",
    "- **$\\mathbf{W}$** - **обучаемая матрица весов** для линейного преобразования\n",
    "- **$h_i, h_j$** - эмбеддинги узлов $i$ и $j$\n",
    "- **$[\\cdot \\parallel \\cdot]$** - **конкатенация** векторов\n",
    "- **$\\mathbf{a}^T$** - **вектор-параметр внимания** (обучаемый)\n",
    "- **LeakyReLU** - функция активации с \"утечкой\"\n",
    "- **$\\mathcal{N}(i)$** - **окрестность узла** $i$ (соседи)\n",
    "- **$\\exp$** - экспонента для softmax\n",
    "\n",
    "#### В формуле обновления:\n",
    "- **$h_i^{(l+1)}$** - **новый эмбеддинг узла** $i$ на слое $l+1$\n",
    "- **$\\sigma$** - функция активации (например, ELU, ReLU)\n",
    "- **$\\sum_{j \\in \\mathcal{N}(i)}$** - **суммирование по всем соседям** узла $i$\n",
    "- **$\\alpha_{ij}$** - веса внимания, вычисленные на предыдущем шаге\n",
    "\n",
    "### Пошаговый процесс:\n",
    "\n",
    "1. **Линейное преобразование**: $\\mathbf{W}h_i$ - преобразуем эмбеддинги каждого узла\n",
    "2. **Конкатенация**: $[\\mathbf{W}h_i \\parallel \\mathbf{W}h_j]$ - объединяем эмбеддинги пары узлов\n",
    "3. **Внимание**: $\\mathbf{a}^T[\\cdot]$ - вычисляем \"сырой\" score внимания\n",
    "4. **Активация**: LeakyReLU - применяем нелинейность\n",
    "5. **Нормализация**: softmax - преобразуем в вероятности внимания\n",
    "6. **Агрегация**: взвешенная сумма соседних эмбеддингов\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Relational GCN (RGCN)\n",
    " \"Наконец, есть специально разработанные свертки для обработки графов, ребра которых могут быть нескольких типов. Одна из них называется RGCN (Relational Graph Convolutional Networks).\"\n",
    "\n",
    "<br><br/>\n",
    "<div align=\"left\">\n",
    "<img src=\"./theory_of_graphs/RGCN.png\" width=\"900\" height=\"400\" align=\"center\">\n",
    "<div/>\n",
    "<div align = 'left'>\n",
    "<br><br/>\n",
    "\n",
    "\n",
    "**Формула RGCN:**\n",
    "$h_i^{(l+1)} = \\sigma\\left(\\sum_{r \\in R}\\sum_{j \\in \\mathcal{N}_i^r} \\frac{1}{c_{i,r}}W_r^{(l)}h_j^{(l)} + W_0^{(l)}h_i^{(l)}\\right)$\n",
    "\n",
    "---\n",
    "\n",
    "## Применение графовых сетей\n",
    "\n",
    "### В различных областях\n",
    "> \"GCN применяют в задачах классификации текстовых данных, изображений, болезней и прогнозировании побочных эффектов; GAT используют для классификации текста, обнаружения объектов на изображениях, задач комбинаторной оптимизации; GGNN применяют для нейронного машинного перевода, для анализа социальных отношений.\"\n",
    "\n",
    "### Пример: Zero-shot learning на картинках\n",
    "> \"Пример - классификация картинок в задаче zero-shot learning (построить классификатор по категории без примеров картинок этой категории). В статье строят граф того, как категории соотносятся друг с другом. Например, кошка - это млекопитающее, значит вершина 'кошка' будет соединена с вершиной 'млекопитающие'.\"\n",
    "\n",
    "### Пример: Обработка текстов\n",
    "> \"При обработке текстов GNN также оказываются весьма полезны. Например, классифицировать текст можно, представив его в виде графа связанных слов - этот способ получить вектора для слов хорош в тех случаях, когда слова не находятся рядом в тексте, но их сходство можно уловить по общим соседям.\"\n",
    "\n",
    "## Проблемы и ограничения\n",
    "\n",
    "### Основные проблемы GNN\n",
    "1. **Ограниченная глубина сетей:**\n",
    "-  *Все графовые сети неглубокие(в районе 3 слоев), при увеличении числа слоев сеть перестает адекватно работать*\n",
    "\n",
    "2. **Динамические графы:**\n",
    "- *Мы не умеем работать с динамическими графами*\n",
    "\n",
    "3. **Вычислительная сложность:**\n",
    "-  *Графовые нейронки очень долго и тяжело учатся по ресурсам, плохо обобщаются*\n",
    "\n",
    "4. **Преобразование данных:**\n",
    "-  *Нет оптимальных методов генерить граф из сырых данных*\n",
    "\n",
    "\n",
    "## Техники улучшения GNN\n",
    "\n",
    "### Нормализация\n",
    "> \"Более стабильной альтернативой BatchNorm в обработке графов, например, являются LayerNorm и GraphNorm\"\n",
    "\n",
    "**GraphNorm:**\n",
    "> $x_i' = \\frac{x - \\alpha \\odot \\mathbb{E}[x]}{\\sqrt{\\text{Var}[x - \\alpha \\odot \\mathbb{E}[x]] + \\epsilon}} \\odot \\gamma + \\beta$\n",
    "\n",
    "\n",
    "- **$x$** - входные активации (эмбеддинги вершин графа)\n",
    "- **$\\mathbb{E}[x]$** - среднее значение активаций по графу\n",
    "- **$\\alpha$** - **обучаемый параметр центрирования** (регулирует силу вычитания среднего)\n",
    "- **$\\odot$** - поэлементное умножение\n",
    "- **$\\text{Var}[\\cdot]$** - дисперсия (мера разброса данных)\n",
    "- **$\\epsilon$** - малая константа для численной стабильности ($\\approx 10^{-5}$)\n",
    "- **$\\gamma$** - **обучаемый параметр масштабирования** (восстанавливает масштаб)\n",
    "- **$\\beta$** - **обучаемый параметр сдвига** (восстанавливает смещение)\n",
    "- **$x_i'$** - нормализованные выходные активации\n",
    "\n",
    "\n",
    "### Skip-connections\n",
    "> \"Для решения этой проблемы придумали skip connection (вспомним ResNet). Идея заключается в добавлении коротких соединений, которые обходят один или несколько слоев нейронной сети, позволяя пропускать входные данные или их преобразованные версии напрямую к последующим слоям.\"\n",
    "\n",
    "### Сэмплирование\n",
    "> \"Когда граф становится огромным, сэмплинг помогает выбрать подмножество узлов или рёбер для обработки. После выбора подмножества данных, распространение информации происходит только на этом подмножестве, что позволяет ускорить вычисления и сделать их более эффективными.\"\n",
    "\n",
    "**Типы сэмплирования:**\n",
    "- Node sampling\n",
    "- Layer sampling  \n",
    "- Subgraph sampling\n",
    "\n",
    "## Инструменты и библиотеки\n",
    "\n",
    "- \"PyTorch Geometric\"\n",
    "\n",
    "- \"Deep Graph Library (DGL)\"\n",
    "\n",
    "- \"Graph Nets\"\n",
    "\n",
    "- \"Spektral\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0251f8d4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5758978",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_TMS_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
